---
title: 'Manual cars more fuel efficient: myth or reality?'
subtitle    : 
author      : Giovanni Fossati
job         : null
output      : 
  html_document:
    self_contained: true
    css: css/gf_small_touches.css
    theme: cerulean
    highlight: tango
    keep_md: yes
    mathjax: default
    toc: no
---


```{r setup, cache=FALSE, echo=FALSE, message=FALSE, warning=FALSE, tidy=FALSE}
# make this an external chunk that can be included in any file
library(knitr)
options(width = 100)
opts_chunk$set(message = FALSE, 
               error = FALSE, 
               warning = FALSE, 
               collapse = TRUE, 
               tidy = FALSE,
               cache = FALSE, 
               cache.path = '.cache/', 
               comment = '#',
               fig.align = 'center', 
               dpi = 100, 
               fig.path = 'figures/')

# options(xtable.type = 'html')
# knit_hooks$set(inline = function(x) {
#   if(is.numeric(x)) {
#     round(x, getOption('digits'))
#   } else {
#     paste(as.character(x), collapse = ', ')
#   }
# })
# knit_hooks$set(plot = knitr:::hook_plot_html)
```

```{r loadLibs, echo=FALSE, cache=FALSE}
library(ggplot2)
library(gridExtra)
```

```{r loadData, echo=FALSE, cache=FALSE, results="hide"}
data(mtcars)
mtcars$gp100m <- 100./mtcars$mpg
mtcars$hp2wt <- mtcars$hp/mtcars$wt
mtcars$amFac <- as.factor(mtcars$am)
mtcars$cylFac <- as.factor(mtcars$cyl)
SAVE.car_names <- row.names(mtcars)
str(mtcars)

set.seed(2468)

source("./scripts/my_defs.R")
source("./scripts/set_colors.R")

# layout1 : 1 | 1 / 2
shape1 <- c(1, 1, 1, 0, 2, 2, 0)
shape2 <- c(1, 1, 1, 3, 3, 4, 4)
shape <- c( rep(shape1, 1), rep(shape2, 1))
mat.layout1 <- matrix(shape, 2, 7, byrow = TRUE)

# layout1 : 1 | 1 / 3 
shape1 <- c(1, 1, 1, 2, 2, 2)
shape2 <- c(3, 3, 4, 4, 5, 5)
shape <- c( rep(shape1, 3), rep(shape2, 2))
mat.layout2 <- matrix(shape, 5, 6, byrow = TRUE)

# layout3 : 1 | 1 / 3 
shape1 <- c(1, 1, 1, 2, 2, 2)
shape2 <- c(3, 3, 4, 4, 5, 5)
shape <- c( rep(shape1, 9), rep(shape2, 6))
mat.layout3 <- matrix(shape, 15, 6, byrow = TRUE)

```

```{r run_models, echo=FALSE, cache=FALSE}
m1 <- lm(mpg ~ wt + amFac + wt*amFac, data = mtcars)

m2a0 <- lm(gp100m ~ wt, data = mtcars)
m2a1 <- lm(gp100m ~ wt + amFac, data = mtcars)
m2a2 <- lm(gp100m ~ wt + amFac + wt*amFac, data = mtcars)

m2b0 <- lm(gp100m ~ wt, data = mtcars)
m2b1 <- lm(gp100m ~ wt + cylFac, data = mtcars)
m2b2 <- lm(gp100m ~ wt + cylFac + wt*cylFac, data = mtcars)

m2c0 <- lm(gp100m ~ wt , data = mtcars)
m2c1 <- lm(gp100m ~ hp , data = mtcars)
m2c2 <- lm(gp100m ~ wt + hp , data = mtcars)
# m2c3 <- lm(gp100m ~ wt + hp + wt:am + hp:am, data = mtcars)
# m2c4 <- lm(gp100m ~ wt + hp + wt:cyl + hp:cyl, data = mtcars)

m2d0 <- lm(gp100m ~ wt , data = mtcars)
m2d1 <- lm(gp100m ~ disp , data = mtcars)
m2d2 <- lm(gp100m ~ wt + disp , data = mtcars)

m3 <- lm(gp100m ~ hp2wt + amFac + hp2wt*amFac, data = mtcars)

m4a0 <- lm(gp100m ~ disp, data = mtcars)
m4a1 <- lm(gp100m ~ disp + amFac, data = mtcars)
m4a2 <- lm(gp100m ~ disp + amFac + disp*amFac, data = mtcars)
```

```{r naive_test, echo=FALSE, cache=FALSE}
naive <- t.test(mpg ~ amFac, data=mtcars, alternative="t")
diff.mpg <- naive$estimate[2]-naive$estimate[1]
diff.sigma <- (naive$conf.int[2]-naive$conf.int[1])/2.
```
```{r scaled_model, echo=FALSE, cache=FALSE}
b.sc <- lm(gp100m ~ I(wt-2) + I(hp/100-1), data=mtcars)
wt2hp <- lm(I(wt - 2) ~ I(hp/100 - 1), data = mtcars)
```

### PREAMBLE

Report for assignment of the [_Regression Models_](https://www.coursera.org/course/regmods) 
course of the Coursera/JHSPH Data Science Specialization.   
The source files are posted at [https://github.com/pedrosan/ReprodRes1](https://github.com/pedrosan/MTcars).


## INTRODUCTION

In this report we address a long-debated and controversial issue concerning the fuel 
economy of automobiles with automatic or manual transmission: 

* Which one is better in terms of MPG?  
* How much is the difference?

We analyzed design and performance data for a sample of 32 car models, european
and american, high-end and economy, and can offer you some interesting points 
clarifying the fuel economy vs. transmission issue beyond common wisdom, urban
legends, and _philosophical_ biases ;-)

## Executive Summary

A naive analysis, _not controlling_ for other car features and performance metrics, 
would suggest that manual cars are more fuel-efficient, by about `r round(diff.mpg,1)` 
miles per gallon, at a `r round(100*(1-naive$p.value),2)`% confidence level 
(_2-sample t-test, 2-sided_).  
However, this assessment does not take into any consideration the heterogeneity
of car models. 
Assuming that the data at our disposal are representative of the general car
"population", our linear regression analysis leads us to conclude that in fact __there is
NOT any significant difference in fuel efficiency between cars with automatic
and manual transmission__.   
Most of the difference (variance) in fuel efficiency can be explained in terms 
of car weight and engine power.  

The apparent better fuel economy of manual transmission emerges from a strong
relationship between type of transmission and several other features (_e.g._
the cars with automatic transmission tend to be heavier, have larger and more
powerful engines).   
Once the effect of these other "physical" factors is removed there is no
residual difference between automatic and manual cars. 
This could actually be appreciated already by looking at the subset of cars in
the narrow middle range of parameter values where both types of cars are present.

In the remainder of the report we summarize the methods and findings.
Plots are presented in the Appendix.    
This report is being produced with a "live" R-markdown document, that
I will post on my [GitHub repo](https://github.com/pedrosan).

## Data, Selection and Processing

The dataset comprises 32 car models.
For each of them it reports its gas mileage in miles per gallon (MPG) and 
10 other features encompassing various aspects of engine/powertrain design
and measures of performance.
Of the 10 potential _predictors_, five can be regarded as categorical either
directly (_e.g._ `am`) or effectively (_e.g._ `carb`), and we treated them as _factors_.
The other five take continuous numeric values.  

**MPG** and **GP100M**: we performed our analysis on a transformed variable instead
of directly on `mpg`, defining _gallons per 100 miles_ (`gp100m <- 100/mpg`).  
We decided to do this because `mpg`'s relationship with most continuous
variables exhibits a marked curvature, which can potentially be an issue given
our intent of working with linear models and hence it would recommend to
transform the variables to mitigate its effect.
The new variable exhibits much more "regular" (_i.e._ straighter) relationships
with the predictors.
Moreover, and likely related to the straightening of the relationships, `gp100m` 
_more transparently_ represents what truly is the most important variable in the
analysis, that is the amount of fuel used by a car.  
It is also more intepretable with respect to many of the predictors: a
_back-of-the-envelope_ argument about $\text{energy} \propto \text{force}
\times \text{distance}$, and in turn $\text{force} \propto \text{weight}$ would
provide an explanation of the correlation between `gp100m` and `wt` (car weight).
Also, correlations of `gp100m` with `disp` (the engine displacement volume) and
`hp` (engine power) could be fairly promptly interpreted because both variables
can be expected to have a proportional effect on fuel efficiency expressed in this way.

We first reviewed the relationship between the 10 variables by means of _pairs
plots_ (see Appendix) and correlation analyses, looking for potentially significant trends,
such as correlations or noticeable differences between the two groups of
_automatic_ and _manual_ transmission cars (coded in the `am` factor).

Based on this preliminary review, for what concerns categorical variables we
decided to limit our analyses to `am` and `cyl` and to leave out `vs`, `gear` and `carb`.
Among the continuous variables we left out `qsec` (the 1/4 mile time) because it did
not show any significant relationship with any other variable, except with `hp` which 
is not surprising given `qsec` depends very directly on short-duration
acceleration, hence engine power.

## Findings

### Exploratory Analysis : General Trends and Correlations

As the __pairs-plot__ shows `gp100m` is strongly correlated with `wt` and `disp`, 
and between the two selected categorical variables `cyl` seems to exert a stronger
influence than `am`.   
This is illustrated also by the __interaction plots__ showing that the mean `gp100m` 

* within each `cyl` group is basically independent on `am`, and that
* its offset between automatic and manuals cars in each `cyl` group is very small.   

The __boxplots__ further highlights that all "important" continuous variables
exhibit systematic differences between `am` groups, similarly to `gp100m`, thus
suggesting that the apparent correlation between these latter may not be truly
a fundamental characteristic of automobiles.


### Linear Regression Models

In this short report we focus on the results of the analysis based on what emerged
as the most important continuous variables, `wt` and `hp`, combined with the two
factor variables `am` and `cyl`.
In particular we discuss three groups of models for explaining the variation in fuel efficiency, all
based on a baseline model with `wt` as sole predictor.  They are:

1. car weight with possible effect of type of transmission (`gp100m ~ wt*am`).
2. car weight with possible effect of number of cylinders (`gp100m ~ wt*cyl`).
3. car weight and engine power (`gp100m ~ wt + hp`).

In each case we look at the model incrementally, for instance including only `wt`, the 
adding `am`, and finally adding the interaction between them (i.e. allowing for
different slopes for each category-group).

In the models of __Case #1__ the addition of `am`, even with interaction, is not accompanied by 
a statistically significant improvement, as it can be summarized 
by the adjusted-$R^2$ metric, which is 
`r round(summary(m2a0)$adj.r.squared, 3)`, 
`r round(summary(m2a1)$adj.r.squared, 3)`.
`r round(summary(m2a2)$adj.r.squared, 3)`, respectively.
The _analysis of variance_ of the nested models gives a $P(>F)$ of 
`r round(anova(m2a0,m2a1,m2a2)[["Pr(>F)"]][2], 3)` and 
`r round(anova(m2a0,m2a1,m2a2)[["Pr(>F)"]][3], 3)`
for the `wt + am` and `wt*am` models.    
The type of transmission has basically no influence on the regression of `gp100m` on `wt`.   
The model is quite good, as illustrated by the _summary diagnostic plots_: 
it is worth noting that the residuals are distributed similarly for both `am` groups,
visually showing that there is no additional "explanatory" effect by `am` after the
effect of `wt` is subtracted.

_Note on MPG_: it is interesting to note that in the case of `mpg` because of
the curvature of its relationship with `wt`, which can be reasonably well
approximated by two branches with different slopes, the addition of the
interaction with `am` improves the model significantly.   Our impression is
that this is an artefact of the less-than-ideal nature of `mpg` as the response
variable. `am` just happens to be a fortunately convenient variable to split
the data in two groups that allow to approximate well the non-linear
relationship between `mpg` and `wt`.   If the dataset had a larger overlap 
between automatic and manual transmission cars, both types extending into the
_other_ end of the dataset, it is likely that `am` will no longer be such a savior.

__Case #2__ looks at the influence of `cyl` instead of `am`.
Again, we compared a sequence of nested models `wt`, `wt + cyl`, `wt*cyl`.
The adjusted-$R^2$ metric values are 
`r round(summary(m2b0)$adj.r.squared, 3)` (same as above, being the same _base model_), 
`r round(summary(m2b1)$adj.r.squared, 3)`.
`r round(summary(m2b2)$adj.r.squared, 3)`, respectively, a modest improvement
over the previous case.
The _analysis of variance_ of the nested models gives a $P(>F)$ of 
`r round(anova(m2b0,m2b1,m2b2)[["Pr(>F)"]][2], 3)` and 
`r round(anova(m2b0,m2b1,m2b2)[["Pr(>F)"]][3], 3)` which suggests that albeit at
low statistical significance, in this case the addition of the factor variable
does improve the model, but the extension with the interaction term 
does not yield any further improvement.

The models in final set, __Case #3__, are based on `wt` and `hp` only, one model
with each of the separately, and one with them combined (no interaction).
The adjusted-$R^2$ values are 
`r round(summary(m2c0)$adj.r.squared, 3)` (this is again the same _base model_), 
`r round(summary(m2c1)$adj.r.squared, 3)`.
`r round(summary(m2c2)$adj.r.squared, 3)`, for `wt`, `hp` and `wt + hp` respectively.
The model with `hp` alone provides a pretty poor fit, but the combined model achieves
the highest reduction in variance of all the models we have tested that have up
to three terms in the linear model (including possible interactions).

The _analysis of variance_ of the nested `wt` and `wt+hp` models gives a $P(>F)$ of 
`r round(anova(m2c0,m2c2)[["Pr(>F)"]][2], 3)` which tells us that indeed the
improvement of the model with addition of `hp` is statistically significant.

It is possible that a model with additional predictors might improve over the
`wt+hp`, however looking at the regression plot and at the residuals plot, it
is clear that there is no room left for a possible significant effect on
`gp100m` explainable with `am`, which is the main issue of interest of this analysis.

The summary of the best model is the following
```{r summary_best_model}
printStats.cpt(m2c2)
```
The model has the form: 
$$\text{gp100m} = `r coef(m2c2)[1]` + `r coef(m2c2)[2]`~\text{wt} + `r coef(m2c2)[3]`~\text{hp}$$ 
or better still, shifting the reference point (_origin_) to `wt=2` and `hp=100` and scaling `hp` to 100:   
$$\text{gp100m} = `r coef(b.sc)[1]` + `r coef(b.sc)[2]`~(\text{wt}-2) + `r coef(b.sc)[3]`~(\text{hp}-100)/100$$
The fuel consumption in _gallons/100miles_ of a car of 2000 lbs (`wt=2`) and 100 HP (`hp=100`) is predicted to be about 
`r round(coef(b.sc)[1],2)` and to increase by 
`r round(coef(b.sc)[2],2)` every 1000 lbs. and by `r round(coef(b.sc)[3],2)` every 100 HP.
It is important to keep in mind that `wt` and `hp` are correlated, with a slope of around `r round(coef(wt2hp)[2],2)`, 
and therefore the increase in gas consumption will generally be a combination of the two components growing in similar proportions.


## APPENDIX : SUPPLEMENTARY MATERIAL

### EXPLORATORY ANALYSIS : GENERAL TRENDS AND CORRELATIONS

#### PAIRS PLOTS

For the subset of variables on which we focused our analysis.   
The upper-right panels show the `y~x` scatterplots, with a `lm` line overlayed. 
Colors correspond to the `am` category, blue for automatic (`am=0`) and red for
manual (`am=1`) (this color coding on `am` applies to all plots in this report).
The lower-left panels report the (absolute) value of the correlation coefficients `cor(x,y)`,
and text and background colors are coded to highlight its values (the color of the frame
of the corresponding `y~x` plot is also colored accordingly).

```{r plots-pairs4, fig.height=7, fig.width=7}
par(mar=c(5, 4, 4, 1)+0.1)
par(oma=c(0, 0, 0, 0))
pairs(mtcars[,c("gp100m","wt","disp","hp","drat","cyl","am")], gap=0.5, pch=21, bg=cvec.fill.am, 
      las=1, panel=mypanel, lower.panel=function(...) panel.cor(..., color.bg=TRUE), main="")
mtext(side=3, "pairs plot with correlation values", outer=TRUE, line=-1.2, font=2)
mtext(side=3, "Points are color-coded on 'am' (0=blue, 1=red) / Dashed lines are 'lm(y~x)' fits.\nCorrelation and scatterplot frames are color-coded on the strength of the correlation", outer=TRUE, line=-1.6, padj=1, cex=0.8, font=1)
```

#### INTERACTION PLOTS: <tt>gp100m</tt>, by <tt>cyl</tt> and <tt>am</tt>

Mean `gp100m` for the six groups resulting from the "interaction" between `am` and `cyl`.


```{r plots-interaction, fig.height=4, fig.width=6}
par(mfrow = c(1, 2), oma=c(0, 0, 0, 0), mar=c(4, 4, 3, 0), las=1)
interaction.plot(mtcars$am, mtcars$cyl, mtcars$gp100m, type="b", 
                 pch=pch.cyl, lwd=2, col=colors.cyl, bg=colors.fill.cyl, 
                 ylim=c(2,10), xaxt="n", xlab="", ylab="gp100m", legend=FALSE)
axis(side=1, at=c(1,2), tck=-0.03, labels=NA)
axis(side=1, at=c(1,2), labels=c("a","m"), lwd=0, line=-0.75)
mtext(side=1, "am", line=1.2, font=1)
legend("topright", bty="o", bg="grey95", x.intersp=1.0, y.intersp=0.8, horiz=FALSE, adj=c(0.6), 
       legend=c("4","6","8"), cex=0.8, pch=pch.cyl, pt.bg=colors.fill.cyl, pt.cex=0.9)

par(mar=c(4, 0, 3, 4))
interaction.plot(mtcars$cyl, mtcars$am, mtcars$gp100m, type="b", yaxt="n", 
                 pch=pch.am, lwd=2, col=colors.am,  bg=colors.fill.am,  
                 ylim=c(2,10), xaxt="n", xlab="", ylab="gp100m", legend=FALSE)
axis(4)
axis(side=1, at=c(1,2,3), tck=-0.03, labels=NA)
axis(side=1, at=c(1,2,3), labels=c("4","6","8"), lwd=0, line=-0.75)
mtext(side=1, "cyl", line=1.2, font=1)
legend("topright", bty="o", bg="grey95", x.intersp=1.0, y.intersp=0.8, horiz=FALSE, adj=c(0.1), 
       legend=c("A (am=0)", "M (am=1)"), cex=0.8, pch=pch.am, pt.bg=colors.fill.am, pt.cex=0.9)

mtext("variation of the mean of gp100m vs. cyl & am", side=3, outer=TRUE, line=-2, font=2)

par(yaxt="s", mfrow = c(1, 1), oma=c(0, 0, 0, 0), mar=c(5, 4, 4, 1)+0.1)
```


#### BOXPLOTS of <tt>gp100m</tt>, <tt>wt</tt>, <tt>hp</tt>, <tt>disp</tt> vs. <tt>am</tt>

```{r plots-boxplots, fig.height=2.5, fig.width=8}
p.base <- theme_bw() + theme(legend.position="none") 

p.gp100m <- ggplot(mtcars, aes(am, gp100m)) + p.base + geom_boxplot(aes(fill=amFac)) + 
                    scale_fill_manual(values = colors.fill.am)
p.wt     <- ggplot(mtcars, aes(am, wt))     + p.base + geom_boxplot(aes(fill=amFac)) + 
                    scale_fill_manual(values = colors.fill.am)
p.hp     <- ggplot(mtcars, aes(am, hp))     + p.base + geom_boxplot(aes(fill=amFac)) + 
                    scale_fill_manual(values = colors.fill.am)
p.disp   <- ggplot(mtcars, aes(am, disp))   + p.base + geom_boxplot(aes(fill=amFac)) + 
                    scale_fill_manual(values = colors.fill.am)

grid.arrange(p.gp100m, p.wt, p.hp, p.disp, ncol=5)
```


<hr />

### LINEAR REGRESSION MODELS

#### 1. <tt> gp100m </tt> vs. <tt> wt </tt> by <tt> am </tt>  (with interaction)

```{r gp100m_vs_wt_by_am-fit, collapse=FALSE}
printStats(m2a0, with.cx=FALSE)
printStats(m2a1, with.cx=FALSE)
printStats(m2a2, with.cx=TRUE)
```

```{r gp100m_vs_wt_by_am-anova}
anova(m2a0, m2a1, m2a2)
```

```{r gp100m_vs_wt_by_am-plots, fig.height=4, fig.width=9}
# layout(matrix(shape, 2, 7, byrow = TRUE))
layout(mat.layout1)

par(mar=c(4, 4, 2, 1)+0.1)
plot(mtcars$wt, mtcars$gp100m, pch=pch.am, col=cvec.am, bg=cvec.fill.am, 
     xlab="wt", ylab="gallons / 100 miles", main="GP100M ~ WT * A/M", 
     cex=1.5)
# abline(c(m2a0$coeff[1],m2a0$coeff[2]), col="green2", lty=4, lwd=3)
addPredConf.am(m2a2, data=mtcars, x="wt", col=colors.am)
legend("topleft", bty="y", bg="grey95", x.intersp=0.7, y.intersp=0.8, legend=c("A (am=0)", "M (am=1)"), 
       cex=1.25, pch=pch.am, pt.bg=colors.fill.am, pt.cex=1.25)

par(mar=c(4, 4, 2, 1)+0.1)
plot(m2a2, which=c(1,2,5), add.smooth=FALSE, pch=pch.am, 
     col=cvec.am, bg=cvec.fill.am, labels.id=1:nrow(mtcars))
```

<hr />

#### 2. <tt> gp100m </tt> vs. <tt> wt </tt> by <tt> cyl </tt>  (with interaction) 

```{r gp100m_vs_wt_by_cyl-fit, collapse=FALSE}
printStats(m2b0, with.cx=FALSE)
printStats(m2b1, with.cx=FALSE)
printStats(m2b2, with.cx=TRUE)
```

```{r gp100m_vs_wt_by_cyl-anova, collapse=FALSE}
anova(m2b0, m2b1, m2b2)
```

```{r gp100m_vs_wt_by_cyl-plots, fig.height=4, fig.width=9}
# layout(matrix(shape, 2, 7, byrow = TRUE))
layout(mat.layout1)

par(mar=c(4, 4, 2, 1)+0.1)
plot(mtcars$wt, mtcars$gp100m, pch=pch.cyl, col=cvec.cyl, bg=cvec.fill.cyl, 
     xlab="wt", ylab="gallons / 100 miles", main="GP100M ~ WT * CYL", cex=1.5)
addPredConf.cyl(m2b2, data=mtcars, x="wt", col=colors.cyl)
legend("topleft", bty="y", bg="grey95", legend=c("4","6","8"), cex=1.25, 
       pch=pch.cyl, pt.bg=colors.fill.cyl, pt.cex=1.25)

par(mar=c(4, 4, 2, 1)+0.1)
plot(m2b2, which=c(1,2,5), add.smooth=FALSE, pch=pch.cyl, col=cvec.cyl, bg=cvec.fill.cyl, 
     labels.id=1:nrow(mtcars))
```

<hr />


#### 3. <tt> gp100m </tt> vs. <tt> wt </tt> & <tt> hp </tt>

```{r gp100m_vs_wt_and_hp-fit, collapse=FALSE}
printStats(m2c0, with.cx=FALSE)
printStats(m2c1, with.cx=FALSE)
printStats(m2c2, with.cx=FALSE)
# printStats(m2c3, with.cx=FALSE)
# printStats(m2c4, with.cx=FALSE)
```

```{r gp100m_vs_wt_and_hp-anova, collapse=FALSE}
anova(m2c0, m2c2)
```

```{r gp100m_vs_wt_and_hp-plots, fig.height=6, fig.width=9}
m2c2.pred.conf <- predict(m2c2, interval="confidence", newdata=mtcars)
cfit1 <- "black"
cfit2 <- rgb(1.0, 1.0, 0.0, 0.3)
par(mar=c(5, 4, 4, 1)+0.1)

# layout(matrix(shape, 5, 6, byrow = TRUE))
layout(mat.layout2)

plot(mtcars$wt, mtcars$gp100m, pch=pch.am, col=cvec.am, bg=cvec.fill.am, xaxt="n", 
     xlab="", ylab="gallons / 100 miles", main="", cex=1.5)
axis(side=1, tck=-0.02, labels=NA)
axis(side=1, lwd=0, line=-0.5)
mtext(side=1, "wt", line=2.0, font=1)
abline(c(m2c0$coeff[1],m2c0$coeff[2]), col=2, lty=2)
points(mtcars$wt, m2c2.pred.conf[,1], pch=23, col=cfit1, bg=cfit2, cex=1.2)
legend("topleft", bty="y", bg="grey95", x.intersp=0.7, y.intersp=0.8, 
       legend=c("A (am=0)", "M (am=1)", "fitted"), cex=1.25, pch=c(pch.am, 23), 
       col=c(colors.am, cfit1), pt.bg=c(colors.fill.am, cfit2), pt.cex=1.25)
mtext(side=3, "GP100M ~ WT + HP", outer=TRUE, line=-1.6, font=2)
mtext(side=3, "Dashed lines show the 'y ~ x' regression. / Diamonds show the prediction of the best LM", 
      outer=TRUE, line=-2.4, padj=1, cex=0.8, font=1)

plot(mtcars$hp, mtcars$gp100m, pch=pch.am, col=cvec.am, bg=cvec.fill.am, xaxt="n", 
     xlab="", ylab="gallons / 100 miles", main="", cex=1.5)
axis(side=1, tck=-0.02, labels=NA)
axis(side=1, lwd=0, line=-0.5)
mtext(side=1, "hp", line=2.0, font=1)
abline(c(m2c1$coeff[1],m2c1$coeff[2]), col=2, lty=2)
points(mtcars$hp, m2c2.pred.conf[,1], pch=23, col=cfit1, bg=cfit2, cex=1.2)
legend("topleft", bty="y", bg="grey95", x.intersp=0.7, y.intersp=0.8, 
       legend=c("A (am=0)", "M (am=1)", "fitted"), cex=1.25, pch=c(pch.am, 23), 
       col=c(colors.am, cfit1), pt.bg=c(colors.fill.am, rgb(1.0, 0.84, 0.0, 0.5)), pt.cex=1.25)

par(mar=c(4, 4, 1, 1))
plot(m2c2, which=c(1,2,5), add.smooth=FALSE, pch=pch.am, col=cvec.am, bg=cvec.fill.am, 
     labels.id=1:nrow(mtcars))
```

<hr />

```{r sessionInfo}
# sessionInfo()
```

## R Session Info

```{r R_session_info}
sessionInfo()
```
---
